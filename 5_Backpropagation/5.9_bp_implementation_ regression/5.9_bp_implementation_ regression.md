[](2019/09/21)

## 5.9 バックプロパゲーションの実装 ─回帰─
- ここでは，バックプロパゲーションによりネットワークが学習する仕組みを理解することを目的とするため，ニューロンや層の数が多くないシンプルなニューラルネットワークを構築する．

<br>

### 5.9.1 回帰の例 ─sin関数の学習─
- ニューラルネットワークにsin関数を学習させることを考える．
- x座標をネットワークへの入力，y座標をネットワークからの出力とする．
- sin関数は連続的な関数のため，このケースは回帰問題になる．
- 出力と正解の誤差を伝播させて，重みとバイアスを修正することを繰り返すことで，ネットワークは少しずつsin関数を学習していく．

<br>

- 今回は，入力層のニューロンが１つ，中間層のニューロンが３つ，出力層のニューロンが１つのシンプルなネットワークを使用する．
- その他設定は以下の通りとする：

|項目|内容|
|:-:|:-:|
|損失関数|二乗和誤差|
|中間層の活性化関数|シグモイド関数|
|出力層の活性化関数|恒等関数|
|最適化アルゴリズム|確率的勾配降下法|
|バッチサイズ|1|

<br>

### 5.9.2 出力層の実装
- 今回は，ニューラルネットワークの各層をクラスとして実装する．
- 以下は，出力層を表すクラス
``` python
class OutputLayer:
    # 初期設定
    def __init__(self, n_upper, n):
        # 重み(行列)とバイアス(ベクトル)
        self.w = wb_width * np.random.randn(n_upper, n)
        self.b = wb_width * np.random.randn(n)

    # 順伝播
    def forward(self, w):
        self.x = x
        u = np.dot(x, self.x) + self.b
        self.y = u # 恒等関数

    # 逆伝播
    def backward(self, t):
        delta = self.y - t

        self.grad_w = np.dot(self.x.T, delta)
        self.grad_b = np.sum(delta, axis=0)

        self.grad_x = np.dot(delta, self.w.T)

    # 重みとバイアスの更新
    def update(self, eta):
        self.w -= eta * self.grad_w
        self.b -= eta * self.grad_b
```

- コンストラクタでは初期設定を行う．上の層のニューロン数(n_upper)，およびこの層のニューロン数(n)を引数として受け取り，重みとバイアスの初期値を設定する．
- 


<br>

### 5.9.3 中間層の実装

<br>

### 5.9.4 バックプロパゲーションの実装

<br>

### 5.9.5 全体のコード

<br>

### 5.9.6 実行結果