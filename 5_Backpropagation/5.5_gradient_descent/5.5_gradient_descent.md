[](2019/05/16)

## 5.5 勾配降下法
- 誤差を次々と前の層へ伝播させて，重みとバイアスを少しずつ更新して最適化するために，勾配降下法というアルゴリズムを使用する．

### 5.5.1 勾配降下法の概要
- あるパラメータ $x_k$ の変化量に対する関数 $y(x1, x2,..., xk,...)$ の変化量の割合，すなわち，勾配 $\frac{\partial y}{\partial x_k}$ を求めて，この勾配に基づいてパラメータを調整し，$y$ を最適化するアルゴリズムを勾配法という．
- 勾配降下法は，勾配法の一種で，結果が $y$ の最小値に向かって降下するようにパラメータ $x_k$ を変化させる．
- <u>バックプロパゲーションにおいては，損失関数により求めた誤差の値を起点に，ニューラルネットワークを遡って重みとバイアスの修正をおこなっていくが，この際に勾配降下法を用いて修正量を決定する．</u>
- <u>勾配降下法では，誤差が小さくなるように，ニューラルネットワークの重みとバイアスを調整する．</u>
- バックプロパゲーションにおける勾配降下法のイメージを次に示す：

<center>
<img src="figures/gradient_descent.png" width="600">
</center>

- 上記グラフでは，横軸の $w_{ij}$ がある重み，縦軸の $E$ が誤差．
- 重みの値に応じて誤差は変化するが，実際は関数の曲線の形状を知ることはできないため，足元の曲線の傾き（勾配）に応じて少しずつ重みを変化させていく．
- この際の各重みの変化量は，この曲線の傾き，すなわち勾配で決まる．（バイアスの場合も同様．）
- <u>したがって，ニューラルネットワークのすべての重みとバイアスを更新するために必要なことは，すべての重みとバイアスに対する，誤差の勾配を求めることになる．</u>

<br>

- なお．$w_{ij}$ の変化に対する $E$ の変化は，必ずしも先程のグラフのようなシンプルな曲線であるとは限らない．
- 次のように，局所的な最小値に囚われて，全体の最小値にたどり着くことができない場合もある：

<center>
<img src="figures/local_minimum.png" width="500">
</center>

- このような最小値を局所最適解という．それに対して，真の最小値を大域最適解という．
- 局所最適解を避けるためには様々な調整が必要になる．

<br>

- 勾配降下法による重みとバイアスの更新は，$w$ を重み，$b$ をバイアス，$E$ を誤差として，偏微分を用いた次の式で表すことができる：

$$
w \leftarrow w - \eta \frac{\partial E}{\partial w}
，b \leftarrow b - \eta \frac{\partial E}{\partial b}．
$$

- $\eta$ は学習係数と呼ばれる定数で，$\frac{\partial E}{\partial w}$ と $\frac{\partial E}{\partial b}$ が勾配．

<br>

- 学習係数は，学習の速度を決める定数．
- 0.1や0.01などの小さな値が使われることが多いが，小さすぎると学習に時間がかかりすぎたり，局所最適解に囚われてしまう，といった問題が発生する．
- しかし，学習係数が大きすぎても，誤差が収束しにくくなるという問題が発生する．
- そのため，効率よく大域最適解にたどり着くためには，学習係数を適切に設定する必要がある．

<br>

### 5.5.2 勾配の求め方の概要
- 勾配さえ求めれば，$\frac{\partial E}{\partial w}$と$\frac{\partial E}{\partial b}$に基づき，重みとバイアスを更新することができる．

<br>

### 5.5.3 出力層の勾配

<br>

### 5.5.4 出力層における入力の勾配

<br>

### 5.5.5 中間層の勾配

<br>

### 5.5.6 勾配を求める式のまとめ

<br>

### 5.5.7 勾配の求め方 ─回帰─

<br>

### 5.5.8 勾配の求め方 ─分類─