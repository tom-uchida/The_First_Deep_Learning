[](2019/04/25)

## 5.5 勾配降下法
- 誤差を次々と前の層へ伝播させて，重みとバイアスを少しずつ更新して最適化するために，
- 勾配降下法というアルゴリズムを使用する．

### 5.5.1 勾配降下法の概要
- あるパラメータxkの変化量に対する関数y(x1, x2,..., xk,...)の変化量，
- すなわち，勾配を求めて，この勾配に基づいてパラメータを調整し，yを最適化するアルゴリズムを勾配法という．
<br></br>
- 勾配降下法は，勾配法の一種で，結果がyの最小値に向かって降下するようにパラメータxkを変化させる．
- 勾配降下法では，誤差が小さくなるように，ニューラルネットワークの重みとバイアスを調整する．
- 実際は関数の曲線の形状を知ることはできないため，足元の曲線の傾き（勾配）に応じて少しずつ重みを変化させていく．
- この際の各重みの変化量は，この曲線の傾き，すなわち勾配で決まる．（バイアスの場合も同様．）
- したがって，ニューラルネットワークのすべての重みとバイアスを更新するために必要なことは，すべての重みとバイアスに対する，誤差の勾配を求めることになる．
- 曲線によっては，局所的な最小値に囚われて，全体の最小値にたどり着くことができない場合もある．
- このような最小値を局所最適解という．それに対して，真の最小値を大域最適解という．
<br></br>
- 勾配降下法による重みとバイアスの更新は，$w$を重み，$b$をバイアス，$E$を誤差として，偏微分を用いた次の式で表すことができる．（$\eta$は学習係数と呼ばれる定数で，$\frac{\partial E}{\partial w}$と$\frac{\partial E}{\partial b}$が勾配．）

$$
w \leftarrow w - \eta \frac{\partial E}{\partial w}
，b \leftarrow b - \eta \frac{\partial E}{\partial b}
$$

### 5.5.2 勾配の求め方の概要
- 勾配さえ求めれば，$\frac{\partial E}{\partial w}$と$\frac{\partial E}{\partial b}$に基づき，重みとバイアスを更新することができる．